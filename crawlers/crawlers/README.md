# Sistema de Crawlers OpenMedia

## üìã √çndice
- [Arquitectura General](#arquitectura-general)
- [√Årbol de Dependencias](#√°rbol-de-dependencias)
- [Componentes Principales](#componentes-principales)
- [Flujo de Trabajo](#flujo-de-trabajo)
- [BaseCrawler - Documentaci√≥n T√©cnica](#basecrawler---documentaci√≥n-t√©cnica)
- [NewsCrawler - Especializaci√≥n](#newscrawler---especializaci√≥n)
- [Utilidades del Sistema (utils.py)](#utilidades-del-sistema-utilspy)
- [Modelos de Datos (models.py)](#modelos-de-datos-modelspy)
- [Configuraci√≥n de Sitios](#configuraci√≥n-de-sitios)
- [Gesti√≥n de Sitios](#gesti√≥n-de-sitios)
- [Extracci√≥n de Contenido](#extracci√≥n-de-contenido)
- [Integraci√≥n con Scrapy](#integraci√≥n-con-scrapy)
- [Manejo de Errores](#manejo-de-errores)
- [M√©tricas y Monitoreo](#m√©tricas-y-monitoreo)

## üèóÔ∏è Arquitectura General

```mermaid
graph TB
    subgraph "Crawler System"
        CR[Crawler Registry<br/>:8080] 
        SM[Site Manager<br/>:8081]
        US[URL Scheduler<br/>:8082]
        
        subgraph "Crawler Instances"
            NC1[News Crawler 1]
            NC2[News Crawler 2]
            BC[Base Crawler]
        end
        
        subgraph "Data Processing"
            K[Kafka<br/>news_content]
            R[Redis<br/>URL Queue]
        end
        
        subgraph "External Services"
            WS[Web Sites]
        end
    end
    
    CR --> NC1
    CR --> NC2
    SM --> NC1
    SM --> NC2
    US --> NC1
    US --> NC2
    
    NC1 --> K
    NC2 --> K
    NC1 --> R
    NC2 --> R
    
    NC1 --> WS
    NC2 --> WS
    
    US --> R
```

## üå≥ √Årbol de Dependencias

```
crawlers/
‚îú‚îÄ‚îÄ base/                           # M√≥dulos base del sistema
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                # Exporta interfaces principales
‚îÇ   ‚îú‚îÄ‚îÄ interfaces.py              # Contratos y abstracciones
‚îÇ   ‚îú‚îÄ‚îÄ models.py                  # Modelos de datos (Article, WebPage)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                   # Utilidades del sistema
‚îÇ   ‚îî‚îÄ‚îÄ content_extractor.py       # Extractor de contenido avanzado
‚îÇ
‚îú‚îÄ‚îÄ crawlers/                      # Implementaciones de crawlers
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # Exporta crawlers disponibles
‚îÇ   ‚îú‚îÄ‚îÄ base_crawler.py           # Crawler base con Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ news_crawler.py           # Crawler especializado en noticias
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Esta documentaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ config/                       # Configuraciones
‚îÇ   ‚îî‚îÄ‚îÄ sites/                    # Configuraciones de sitios
‚îÇ       ‚îî‚îÄ‚îÄ chile_news.json       # Sitios chilenos configurados
‚îÇ
‚îú‚îÄ‚îÄ services/                     # Servicios externos
‚îÇ   ‚îú‚îÄ‚îÄ registry/                 # Registro de crawlers
‚îÇ   ‚îú‚îÄ‚îÄ scheduler/                # Programador de URLs
‚îÇ   ‚îú‚îÄ‚îÄ site_manager/             # Gestor de sitios
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/               # Monitoreo y m√©tricas
‚îÇ
‚îî‚îÄ‚îÄ scripts/                      # Scripts de utilidad
    ‚îî‚îÄ‚îÄ start-crawling.sh         # Script de inicio
```

### Dependencias entre M√≥dulos

```mermaid
graph TD
    subgraph "Base Layer"
        I[interfaces.py]
        M[models.py]
        U[utils.py]
        CE[content_extractor.py]
    end
    
    subgraph "Crawler Layer"
        BC[base_crawler.py]
        NC[news_crawler.py]
    end
    
    subgraph "Configuration"
        SC[sites config]
    end
    
    subgraph "External Services"
        R[Registry]
        SM[Site Manager]
        US[URL Scheduler]
        K[Kafka]
    end
    
    I --> BC
    I --> NC
    M --> CE
    M --> NC
    U --> BC
    U --> NC
    CE --> NC
    BC --> NC
    
    SC --> SM
    SM --> BC
    SM --> NC
    
    R --> BC
    R --> NC
    US --> BC
    US --> NC
    K --> BC
    K --> NC
```

## üîß Componentes Principales

### 1. BaseCrawler (`base_crawler.py`)
**Clase base abstracta que implementa la interfaz `ICrawler`**

#### Responsabilidades:
- ‚úÖ Gesti√≥n del ciclo de vida del crawler
- ‚úÖ Comunicaci√≥n con servicios externos (Registry, Site Manager, Scheduler)
- ‚úÖ Integraci√≥n con Scrapy framework
- ‚úÖ Manejo de m√©tricas y heartbeats
- ‚úÖ Procesamiento de requests y responses

### 2. NewsCrawler (`news_crawler.py`)
**Especializaci√≥n para sitios de noticias**

#### Responsabilidades:
- ‚úÖ Extracci√≥n espec√≠fica de art√≠culos de noticias
- ‚úÖ Validaci√≥n de contenido period√≠stico
- ‚úÖ Env√≠o a pipeline de procesamiento
- ‚úÖ Manejo de metadatos espec√≠ficos de noticias

### 3. OpenMediaSpider (dentro de `base_crawler.py`)
**Spider de Scrapy personalizado**

#### Responsabilidades:
- ‚úÖ Parsing de HTML con selectores configurables
- ‚úÖ Extracci√≥n de links y contenido
- ‚úÖ Env√≠o a Kafka
- ‚úÖ Manejo de delays y pol√≠ticas de crawling

## üîÑ Flujo de Trabajo

```mermaid
sequenceDiagram
    participant C as Crawler Instance
    participant R as Registry
    participant SM as Site Manager
    participant US as URL Scheduler
    participant S as Scrapy Spider
    participant K as Kafka
    
    Note over C: Inicializaci√≥n
    C->>R: register_crawler()
    C->>SM: load_site_configs()
    C->>C: initialize_scrapy()
    
    Note over C: Loop Principal
    loop Crawling Loop
        C->>US: get_next_url()
        US-->>C: CrawlRequest
        C->>S: process_with_scrapy()
        S->>S: parse_response()
        S->>S: extract_content()
        S->>K: send_to_kafka()
        S->>S: extract_links()
        C->>US: report_success/failure()
        C->>R: send_heartbeat()
    end
    
    Note over C: Finalizaci√≥n
    C->>R: unregister_crawler()
```

## üìö BaseCrawler - Documentaci√≥n T√©cnica

### Estructura de Clases

```python
class BaseCrawler(ICrawler):
    """
    Implementaci√≥n base del crawler usando Scrapy framework
    """
```

### Atributos Principales

| Atributo | Tipo | Descripci√≥n |
|----------|------|-------------|
| `crawler_id` | `str` | UUID √∫nico del crawler |
| `crawler_type` | `str` | Tipo de crawler (base, news, etc.) |
| `status` | `CrawlerStatus` | Estado actual del crawler |
| `config` | `Dict[str, Any]` | Configuraci√≥n del crawler |
| `site_configs` | `Dict[str, SiteConfig]` | Configuraciones de sitios asignados |
| `metrics` | `Dict[str, Any]` | M√©tricas de rendimiento |
| `runner` | `CrawlerRunner` | Runner de Scrapy |

### M√©todos Principales

#### üöÄ Inicializaci√≥n

```python
def initialize(self, config: Dict[str, Any]) -> bool:
    """
    Inicializa el crawler con configuraci√≥n
    
    Flujo:
    1. Actualiza configuraci√≥n interna
    2. Configura settings de Scrapy
    3. Inicializa CrawlerRunner
    4. Se registra en Registry
    5. Carga configuraciones de sitios
    """
```

**Settings de Scrapy configurados:**
```python
settings = {
    'ROBOTSTXT_OBEY': True,           # Respeta robots.txt
    'DOWNLOAD_DELAY': 1.0,            # Delay entre requests
    'RANDOMIZE_DOWNLOAD_DELAY': 0.5,  # Randomizaci√≥n del delay
    'CONCURRENT_REQUESTS': 1,         # Requests concurrentes
    'CONCURRENT_REQUESTS_PER_DOMAIN': 1,  # Por dominio
    'USER_AGENT': 'OpenMedia-Crawler/1.0',
    'TELNETCONSOLE_ENABLED': False,
    'LOG_LEVEL': 'INFO'
}
```

#### üîÑ Loop Principal

```python
def run(self):
    """
    Loop principal del crawler
    
    Flujo:
    1. Cambia status a RUNNING
    2. Mientras status == RUNNING:
       a. Obtiene pr√≥xima URL del scheduler
       b. Procesa request con Scrapy
       c. Env√≠a heartbeat al registry
       d. Maneja errores y reintentos
    """
```

#### üï∑Ô∏è Procesamiento con Scrapy

```python
def _process_request_with_scrapy(self, request: CrawlRequest):
    """
    Procesa request usando Scrapy Spider
    
    Flujo:
    1. Obtiene configuraci√≥n del sitio
    2. Crea instancia de OpenMediaSpider
    3. Configura spider con site_config
    4. Crea Scrapy Request
    5. Ejecuta crawling (simulado por ahora)
    """
```

#### üìä Gesti√≥n de M√©tricas

```python
def get_metrics(self) -> Dict[str, Any]:
    """
    Retorna m√©tricas de rendimiento
    
    M√©tricas incluidas:
    - requests_made: Total de requests realizados
    - requests_successful: Requests exitosos
    - requests_failed: Requests fallidos
    - urls_extracted: URLs extra√≠das
    - start_time: Tiempo de inicio
    - last_request_time: √öltimo request
    - status: Estado actual
    - assigned_sites: Sitios asignados
    """
```

## üóûÔ∏è NewsCrawler - Especializaci√≥n

### M√©todos Espec√≠ficos

#### üìÑ Extracci√≥n de Art√≠culos

```python
async def extract_content(self, url: str, html: str, site_config: Dict[str, Any]) -> Optional[Article]:
    """
    Extrae contenido de art√≠culo desde HTML
    
    Proceso:
    1. Obtiene selectores espec√≠ficos del sitio
    2. Usa ContentExtractor para extraer:
       - T√≠tulo
       - Contenido
       - Autor
       - Fecha de publicaci√≥n
       - Resumen
    3. Valida si es un art√≠culo v√°lido
    4. Retorna objeto Article o None
    """
```

#### ‚úÖ Validaci√≥n de Art√≠culos

```python
def _is_valid_article(self, article: Article) -> bool:
    """
    Valida si el contenido extra√≠do es un art√≠culo v√°lido
    
    Criterios:
    - Contenido m√≠nimo: 100 caracteres
    - T√≠tulo m√≠nimo: 10 caracteres
    - No contiene indicadores de p√°ginas no-art√≠culo:
      * privacy policy, terms of service
      * about us, contact us
      * subscribe, newsletter
    """
```

#### üì§ Procesamiento de Resultados

```python
async def process_crawl_result(self, result: CrawlResult) -> None:
    """
    Procesa resultado del crawling
    
    Acciones:
    1. Env√≠a contenido a Kafka (topic: content.extracted)
    2. Actualiza estad√≠sticas de crawling
    3. Maneja errores de procesamiento
    """
```

## üõ†Ô∏è Utilidades del Sistema (utils.py)

El m√≥dulo `utils.py` proporciona funciones esenciales para el funcionamiento del sistema de crawlers.

### Funciones de Configuraci√≥n

#### üîß setup_logging()
```python
def setup_logging(level: str = "INFO", service_name: str = "crawler") -> None:
    """
    Configura logging estructurado para el servicio
    
    Caracter√≠sticas:
    - Logging estructurado con structlog
    - Formato JSON para logs
    - Timestamps ISO
    - Context de servicio
    - Stack traces autom√°ticos
    """
```

#### ‚öôÔ∏è get_config()
```python
def get_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Carga configuraci√≥n desde archivo o variables de entorno
    
    Configuraciones incluidas:
    - Kafka: bootstrap_servers, topic_prefix
    - Redis: host, port, db, password
    - PostgreSQL: host, port, database, credentials
    - Crawler: user_agent, delays, timeouts
    - Service: host, port, debug mode
    
    Prioridad: ENV vars > archivo config
    """
```

### Funciones de URL

#### ‚úÖ validate_url()
```python
def validate_url(url: str) -> bool:
    """Valida formato de URL usando validators library"""
```

#### üåê extract_domain()
```python
def extract_domain(url: str) -> str:
    """
    Extrae dominio principal de URL
    
    Ejemplo:
    - https://www.emol.com/news/article -> emol.com
    - https://subdomain.latercera.com -> latercera.com
    """
```

#### üîÑ normalize_url()
```python
def normalize_url(url: str, base_url: Optional[str] = None) -> str:
    """
    Normaliza URL eliminando fragmentos y par√°metros
    
    Proceso:
    1. Convierte URLs relativas a absolutas
    2. Normaliza esquema y dominio
    3. Elimina fragmentos (#)
    4. Mantiene query parameters
    """
```

### Funciones de Politeness

#### ‚è±Ô∏è calculate_crawl_delay()
```python
def calculate_crawl_delay(site_config: Dict[str, Any], default_delay: float = 1.0) -> float:
    """
    Calcula delay apropiado basado en configuraci√≥n y rate limits
    
    L√≥gica:
    - Toma delay configurado del sitio
    - Calcula delay m√≠nimo basado en rate_limit
    - Retorna el mayor de ambos valores
    
    Ejemplo:
    - rate_limit: 30 req/min -> min_delay: 2.0s
    - crawl_delay: 1.5s -> resultado: 2.0s
    """
```

#### üö´ is_allowed_domain()
```python
def is_allowed_domain(url: str, allowed_domains: list) -> bool:
    """
    Verifica si dominio de URL est√° en lista permitida
    
    Soporta:
    - Dominios exactos: emol.com
    - Subdominios: *.emol.com
    - Matching bidireccional
    """
```

### Funciones de Robots.txt

#### ü§ñ parse_robots_txt()
```python
def parse_robots_txt(robots_content: str, user_agent: str = "*") -> Dict[str, Any]:
    """
    Parsea contenido de robots.txt
    
    Extrae:
    - Rutas permitidas (Allow)
    - Rutas prohibidas (Disallow)
    - Crawl delay espec√≠fico
    - URLs de sitemaps
    
    Retorna:
    {
        'allowed': ['/news/', '/articles/'],
        'disallowed': ['/admin/', '/private/'],
        'crawl_delay': 2.0,
        'sitemap': ['https://site.com/sitemap.xml']
    }
    """
```

#### ‚úÖ is_robots_allowed()
```python
def is_robots_allowed(url: str, robots_rules: Dict[str, Any]) -> bool:
    """
    Verifica si URL est√° permitida seg√∫n robots.txt
    
    L√≥gica:
    1. Verifica allows expl√≠citos primero
    2. Luego verifica disallows
    3. Por defecto permite si no hay reglas
    """
```

### Funciones de Utilidad

#### üñ•Ô∏è get_host_info()
```python
def get_host_info() -> Dict[str, str]:
    """
    Obtiene informaci√≥n del host actual
    
    Retorna:
    {
        'hostname': 'crawler-node-1',
        'platform': 'Linux-5.4.0-x86_64',
        'python_version': '3.9.7',
        'architecture': '64bit'
    }
    """
```

#### üßπ sanitize_filename()
```python
def sanitize_filename(filename: str) -> str:
    """
    Sanitiza nombre de archivo para almacenamiento seguro
    
    - Reemplaza caracteres inv√°lidos con '_'
    - Limita longitud a 255 caracteres
    - Mantiene extensi√≥n si existe
    """
```

#### üï∑Ô∏è create_user_agent()
```python
def create_user_agent(service_name: str = "OpenMedia-Crawler", version: str = "1.0") -> str:
    """
    Crea User-Agent apropiado
    
    Formato: "OpenMedia-Crawler/1.0 (+https://github.com/openmedia/crawler)"
    """
```

## üìä Modelos de Datos (models.py)

Define las estructuras de datos principales del sistema.

### Article (Art√≠culo de Noticias)

```python
@dataclass
class Article:
    """Representa un art√≠culo de noticias extra√≠do"""
    
    # Campos obligatorios
    title: str                          # T√≠tulo del art√≠culo
    content: str                        # Contenido principal
    url: str                           # URL original
    
    # Campos opcionales
    author: Optional[str] = None        # Autor del art√≠culo
    published_date: Optional[datetime] = None  # Fecha de publicaci√≥n
    summary: Optional[str] = None       # Resumen/bajada
    category: Optional[str] = None      # Categor√≠a (pol√≠tica, deportes, etc.)
    tags: List[str] = field(default_factory=list)  # Tags/etiquetas
    
    # Metadatos
    metadata: Dict[str, Any] = field(default_factory=dict)  # Datos adicionales
    extracted_at: datetime = field(default_factory=datetime.utcnow)  # Timestamp extracci√≥n
```

#### M√©todos del Article

```python
def dict(self) -> Dict[str, Any]:
    """
    Convierte a diccionario para serializaci√≥n
    
    Uso:
    - Env√≠o a Kafka
    - Almacenamiento en base de datos
    - APIs REST
    
    Maneja autom√°ticamente:
    - Conversi√≥n de datetime a ISO string
    - Serializaci√≥n de campos opcionales
    """
```

### WebPage (P√°gina Web General)

```python
@dataclass
class WebPage:
    """Representa una p√°gina web extra√≠da (no espec√≠ficamente noticias)"""
    
    # Campos obligatorios
    title: str                          # T√≠tulo de la p√°gina
    content: str                        # Contenido principal
    url: str                           # URL original
    
    # Campos opcionales
    description: Optional[str] = None   # Meta description
    keywords: List[str] = field(default_factory=list)  # Keywords/meta tags
    language: Optional[str] = None      # Idioma detectado
    
    # Metadatos
    metadata: Dict[str, Any] = field(default_factory=dict)
    extracted_at: datetime = field(default_factory=datetime.utcnow)
```

### CrawlResult (Re-exportado)

```python
# Re-exportado desde interfaces.py
from .interfaces import CrawlResult
```

### Uso de los Modelos

#### En NewsCrawler:
```python
# Crear art√≠culo
article = Article(
    title="T√≠tulo extra√≠do",
    content="Contenido del art√≠culo...",
    url="https://emol.com/article/123",
    author="Juan P√©rez",
    category="Pol√≠tica"
)

# Serializar para Kafka
article_data = article.dict()
```

#### En ContentExtractor:
```python
# Extraer y crear modelo
article = self.content_extractor.extract_article(
    html=response.text,
    url=response.url,
    title_selector=selectors.get('title'),
    content_selector=selectors.get('content')
)
```

## ‚öôÔ∏è Configuraci√≥n de Sitios

### Estructura SiteConfig

```python
@dataclass
class SiteConfig:
    domain: str                    # Dominio del sitio
    name: str                     # Nombre descriptivo
    base_urls: List[str]          # URLs base para iniciar
    allowed_domains: List[str]    # Dominios permitidos
    crawl_delay: float = 1.0      # Delay entre requests
    concurrent_requests: int = 1   # Requests concurrentes
    user_agent: str               # User agent personalizado
    respect_robots_txt: bool = True
    custom_headers: Dict[str, str] # Headers personalizados
    selectors: Dict[str, str]     # Selectores CSS/XPath
    rate_limit: int = 60          # Requests por minuto
    priority: int = 1             # Prioridad del sitio
    enabled: bool = True          # Sitio habilitado
```

### Selectores Configurables

```python
selectors = {
    'title': 'h1.article-title',           # T√≠tulo del art√≠culo
    'content': '.article-content p',       # Contenido principal
    'author': '.author-name',              # Autor
    'date': '.publish-date',               # Fecha de publicaci√≥n
    'category': '.category-tag',           # Categor√≠a
    'summary': '.article-summary'          # Resumen
}
```

## üèóÔ∏è Gesti√≥n de Sitios

### Archivo de Configuraci√≥n: `config/sites/chile_news.json`

El sistema utiliza archivos JSON para configurar los sitios a crawlear. Actualmente solo tenemos sitios chilenos configurados.

#### Estructura del Archivo

```json
{
  "sites": [
    {
      "domain": "emol.com",
      "name": "El Mercurio Online",
      "base_urls": [
        "https://www.emol.com/",
        "https://www.emol.com/noticias/"
      ],
      "allowed_domains": [
        "emol.com",
        "www.emol.com"
      ],
      "selectors": {
        "title": "h1.titular::text",
        "content": "div.cuerpo-noticia p::text",
        "author": "span.autor::text",
        "date": "time::attr(datetime)",
        "category": "nav.breadcrumb a:last-child::text"
      },
      "crawl_delay": 2.0,
      "concurrent_requests": 2,
      "rate_limit": 30,
      "priority": 1,
      "enabled": true,
      "respect_robots_txt": true,
      "user_agent": "OpenMedia-Crawler/1.0",
      "custom_headers": {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "es-CL,es;q=0.8,en;q=0.6"
      }
    }
  ]
}
```

### Sitios Chilenos Configurados

| Sitio | Dominio | Delay | Rate Limit | Prioridad | Estado |
|-------|---------|-------|------------|-----------|--------|
| **El Mercurio Online** | emol.com | 2.0s | 30 req/min | 1 | ‚úÖ Activo |
| **La Tercera** | latercera.com | 1.5s | 40 req/min | 1 | ‚úÖ Activo |
| **BioBio Chile** | biobiochile.cl | 1.0s | 50 req/min | 2 | ‚úÖ Activo |
| **Radio Cooperativa** | cooperativa.cl | 1.5s | 35 req/min | 2 | ‚úÖ Activo |

### C√≥mo Modificar Sitios

#### ‚ûï Agregar un Nuevo Sitio

1. **Editar el archivo de configuraci√≥n:**
```bash
nano crawlers/config/sites/chile_news.json
```

2. **Agregar nueva configuraci√≥n:**
```json
{
  "domain": "nuevositio.cl",
  "name": "Nuevo Sitio de Noticias",
  "base_urls": [
    "https://nuevositio.cl/",
    "https://nuevositio.cl/noticias/"
  ],
  "allowed_domains": [
    "nuevositio.cl"
  ],
  "selectors": {
    "title": "h1.title::text",
    "content": ".article-content p::text",
    "author": ".author::text",
    "date": ".date::attr(datetime)",
    "category": ".category::text"
  },
  "crawl_delay": 1.5,
  "concurrent_requests": 2,
  "rate_limit": 40,
  "priority": 2,
  "enabled": true,
  "respect_robots_txt": true,
  "user_agent": "OpenMedia-Crawler/1.0",
  "custom_headers": {
    "Accept-Language": "es-CL,es;q=0.8,en;q=0.6"
  }
}
```

#### ‚ùå Deshabilitar un Sitio

Para deshabilitar temporalmente un sitio sin eliminarlo:

```json
{
  "domain": "sitio.com",
  "name": "Sitio a Deshabilitar",
  // ... otras configuraciones ...
  "enabled": false  // Cambiar a false
}
```

#### üóëÔ∏è Eliminar un Sitio

Para eliminar completamente un sitio:

1. **Eliminar del archivo JSON:**
   - Abrir `crawlers/config/sites/chile_news.json`
   - Eliminar todo el objeto del sitio
   - Guardar el archivo

2. **Reiniciar los crawlers:**
```bash
cd crawlers
./scripts/start-crawling.sh
```

#### üîß Modificar Configuraci√≥n de Sitio Existente

**Ejemplo: Cambiar selectores de Emol**

```json
{
  "domain": "emol.com",
  "selectors": {
    "title": "h1.new-title-class::text",        // Nuevo selector
    "content": "div.new-content-class p::text", // Nuevo selector
    "author": "span.new-author-class::text",    // Nuevo selector
    "date": "time.new-date-class::attr(datetime)",
    "category": "nav.breadcrumb a:last-child::text"
  }
}
```

### Eliminar Sitios Internacionales

Si hubiera sitios internacionales configurados, aqu√≠ est√° c√≥mo eliminarlos:

#### üåç Identificar Sitios Internacionales

Buscar en el archivo de configuraci√≥n sitios con dominios no chilenos:

```bash
# Buscar dominios que no sean .cl
grep -v "\.cl" crawlers/config/sites/chile_news.json
```

#### üóëÔ∏è Proceso de Eliminaci√≥n

1. **Backup del archivo actual:**
```bash
cp crawlers/config/sites/chile_news.json crawlers/config/sites/chile_news.json.backup
```

2. **Editar y eliminar sitios internacionales:**
```bash
nano crawlers/config/sites/chile_news.json
```

3. **Eliminar objetos completos de sitios como:**
```json
// ELIMINAR ESTOS TIPOS DE CONFIGURACIONES
{
  "domain": "cnn.com",           // Sitio internacional
  "domain": "bbc.co.uk",         // Sitio internacional
  "domain": "elpais.com",        // Sitio internacional
  // ... eliminar todo el objeto
}
```

4. **Mantener solo sitios chilenos:**
```json
{
  "sites": [
    // Solo mantener sitios con dominios .cl o chilenos conocidos
    {
      "domain": "emol.com",      // ‚úÖ Mantener
      // ...
    },
    {
      "domain": "latercera.com", // ‚úÖ Mantener
      // ...
    }
    // Eliminar todos los dem√°s
  ]
}
```

### Validaci√≥n de Configuraci√≥n

#### üîç Verificar Sintaxis JSON

```bash
# Validar que el JSON est√© bien formado
python -m json.tool crawlers/config/sites/chile_news.json
```

#### ‚úÖ Verificar Selectores

Para probar selectores en un sitio:

```python
# Script de prueba
import requests
from bs4 import BeautifulSoup

url = "https://www.emol.com/noticias/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Probar selector de t√≠tulo
title = soup.select_one("h1.titular")
print(f"T√≠tulo encontrado: {title.text if title else 'No encontrado'}")
```

### Aplicar Cambios

#### üîÑ Recargar Configuraci√≥n

Los cambios en la configuraci√≥n requieren reiniciar los servicios:

```bash
# Detener crawlers
docker-compose down news-crawler-1 news-crawler-2

# Reiniciar con nueva configuraci√≥n
./scripts/start-crawling.sh
```

#### üìä Verificar Cambios

```bash
# Verificar que los crawlers cargaron la nueva configuraci√≥n
curl http://localhost:8080/crawlers

# Verificar sitios cargados
curl http://localhost:8081/sites
```

## üîç Extracci√≥n de Contenido

### OpenMediaSpider - M√©todos de Parsing

#### üéØ M√©todo Principal de Parsing

```python
def parse(self, response):
    """
    M√©todo principal de parsing de Scrapy
    
    Flujo:
    1. Obtiene configuraci√≥n del sitio
    2. Extrae contenido usando selectores
    3. Env√≠a contenido a Kafka
    4. Extrae y sigue links
    5. Reporta √©xito/fallo al crawler
    """
```

#### üìù Extracci√≥n de Contenido

```python
def _extract_content(self, response, site_config: SiteConfig) -> Dict[str, Any]:
    """
    Extrae contenido usando selectores espec√≠ficos del sitio
    
    Datos extra√≠dos:
    - url: URL de la p√°gina
    - site_id: ID del sitio
    - site_name: Nombre del sitio
    - domain: Dominio
    - title: T√≠tulo extra√≠do
    - content: Contenido principal
    - author: Autor (si disponible)
    - publish_date: Fecha de publicaci√≥n
    - category: Categor√≠a
    - timestamp: Timestamp de extracci√≥n
    - crawler_id: ID del crawler
    - status_code: C√≥digo de respuesta HTTP
    - content_length: Longitud del contenido
    - language: Idioma detectado
    """
```

#### üîó Extracci√≥n de Links

```python
def _extract_links(self, response, site_config: SiteConfig) -> List[str]:
    """
    Extrae todos los links de la p√°gina
    
    Proceso:
    1. Busca todos los elementos <a href="">
    2. Convierte URLs relativas a absolutas
    3. Normaliza URLs
    4. Elimina duplicados
    5. Retorna lista de URLs √∫nicas
    """
```

#### ‚úÖ Filtrado de Links

```python
def _should_follow_link(self, url: str, site_config: SiteConfig) -> bool:
    """
    Determina si un link debe ser seguido
    
    Criterios:
    1. Dominio est√° en allowed_domains
    2. No es archivo binario (pdf, jpg, etc.)
    3. No es p√°gina de sistema (admin, login, etc.)
    4. Cumple con patrones espec√≠ficos del sitio
    """
```

### ContentExtractor - Extracci√≥n Avanzada

#### üéØ Extracci√≥n con Selectores

```python
def _extract_text_by_selector(self, soup: BeautifulSoup, selector: Optional[str]) -> Optional[str]:
    """
    Extrae texto usando selector CSS
    
    Sintaxis especial soportada:
    - 'h1::text' - Extrae solo texto
    - 'meta::attr(content)' - Extrae atributo
    - 'div.class' - Selector CSS normal
    """
```

#### üîÑ M√©todos Fallback

```python
def _extract_title_fallback(self, soup: BeautifulSoup) -> Optional[str]:
    """
    Extrae t√≠tulo usando m√©todos fallback
    
    Orden de prioridad:
    1. h1
    2. title
    3. og:title
    4. twitter:title
    5. .title, .headline
    """

def _extract_content_fallback(self, soup: BeautifulSoup) -> Optional[str]:
    """
    Extrae contenido usando m√©todos fallback
    
    Proceso:
    1. Elimina elementos no deseados (script, style, nav)
    2. Busca en selectores comunes:
       - article
       - .content, .article-content
       - .post-content, .entry-content
       - main, #content
    3. Valida longitud m√≠nima (100 caracteres)
    """
```

## üîÑ Integraci√≥n con Scrapy

### Configuraci√≥n del Spider

```python
class OpenMediaSpider(scrapy.Spider):
    name = 'openmedia_spider'
    
    def __init__(self, crawler_instance, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.crawler_instance = crawler_instance  # Referencia al crawler
        self.site_configs = {}                    # Configuraciones de sitios
        self.kafka_producer = None                # Productor de Kafka
```

### Inicializaci√≥n de Kafka

```python
def _init_kafka(self):
    """
    Inicializa productor de Kafka
    
    Configuraci√≥n:
    - bootstrap_servers: kafka:9092
    - value_serializer: JSON
    - key_serializer: String
    """
```

### Env√≠o a Kafka

```python
def _send_to_kafka(self, content_data: Dict[str, Any]):
    """
    Env√≠a contenido extra√≠do a Kafka
    
    Topic: news_content
    Key: site_id
    Value: content_data (JSON)
    """
```

## ‚ö†Ô∏è Manejo de Errores

### Estrategias de Retry

```python
@dataclass
class CrawlRequest:
    retry_count: int = 0      # Intentos actuales
    max_retries: int = 3      # M√°ximo de reintentos
```

### Tipos de Errores Manejados

1. **Errores de Red**
   - Timeout de conexi√≥n
   - DNS resolution failed
   - Connection refused

2. **Errores HTTP**
   - 4xx: Client errors
   - 5xx: Server errors
   - Rate limiting (429)

3. **Errores de Parsing**
   - HTML malformado
   - Selectores no encontrados
   - Encoding issues

4. **Errores de Servicios**
   - Registry no disponible
   - Scheduler no responde
   - Kafka no accesible

### Reportes de Error

```python
def _report_failure(self, url: str, error_message: str):
    """
    Reporta fallo al scheduler
    
    Payload:
    {
        'url': url,
        'success': False,
        'error_message': error_message
    }
    """
```

## üìä M√©tricas y Monitoreo

### M√©tricas del Crawler

```python
metrics = {
    'requests_made': 0,           # Total requests realizados
    'requests_successful': 0,     # Requests exitosos
    'requests_failed': 0,         # Requests fallidos
    'urls_extracted': 0,          # URLs extra√≠das
    'start_time': None,           # Tiempo de inicio
    'last_request_time': None,    # √öltimo request
    'status': 'running',          # Estado actual
    'crawler_id': 'uuid',         # ID del crawler
    'crawler_type': 'news',       # Tipo de crawler
    'assigned_sites': ['site1']   # Sitios asignados
}
```

### Heartbeat al Registry

```python
def _send_heartbeat(self):
    """
    Env√≠a heartbeat al registry cada ciclo
    
    Endpoint: POST /crawlers/{crawler_id}/heartbeat
    Payload: {'metrics': metrics}
    """
```

### Endpoints de Monitoreo

- **Registry Status**: `curl http://localhost:8080/status`
- **Health Check**: `curl http://localhost:8080/health`
- **Crawler Metrics**: `curl http://localhost:8083/metrics`

## üîß Estados del Crawler

```python
class CrawlerStatus(Enum):
    IDLE = "idle"         # Inactivo, esperando trabajo
    RUNNING = "running"   # Ejecutando crawling
    PAUSED = "paused"     # Pausado temporalmente
    ERROR = "error"       # Error fatal
    STOPPED = "stopped"   # Detenido gracefully
```

### Transiciones de Estado

```mermaid
stateDiagram-v2
    [*] --> IDLE: initialize()
    IDLE --> RUNNING: run()
    RUNNING --> PAUSED: pause()
    PAUSED --> RUNNING: resume()
    RUNNING --> STOPPED: stop()
    PAUSED --> STOPPED: stop()
    RUNNING --> ERROR: fatal_error()
    ERROR --> STOPPED: stop()
    STOPPED --> [*]
```

## üöÄ Uso y Configuraci√≥n

### Inicializaci√≥n de un Crawler

```python
# Crear instancia
crawler = NewsCrawler(crawler_id="news-1", config=config)

# Inicializar
success = crawler.initialize(config)

# Ejecutar
if success:
    crawler.run()
```

### Configuraci√≥n de Sitio

```python
site_config = SiteConfig(
    domain="example.com",
    name="Example News",
    base_urls=["https://example.com/news"],
    allowed_domains=["example.com"],
    selectors={
        'title': 'h1.headline',
        'content': '.article-body p',
        'author': '.byline .author',
        'date': '.publish-date'
    }
)
```

---

## üìù Notas de Implementaci√≥n

1. **Scrapy Integration**: Actualmente simulada, requiere integraci√≥n completa con Twisted reactor
2. **Async Support**: NewsCrawler usa async/await, BaseCrawler es s√≠ncrono
3. **Error Recovery**: Implementar circuit breaker para servicios externos
4. **Scaling**: Considerar sharding de URLs por dominio
5. **Monitoring**: Integrar con Prometheus/Grafana para m√©tricas avanzadas

---

*Documentaci√≥n generada para OpenMedia Crawler System v1.0* 